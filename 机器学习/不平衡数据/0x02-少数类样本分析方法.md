在关注多数类分布来减少多数类样本数量, 减少数据集的不平衡程度之外, **分析少数类样本的分布和它们之间的关系也许能带来更多的帮助**.

分析少数类样本, 获得少数类样本的结构信息, 并将其作为background knowledge来指导整个学习分类器的过程.

## 分析角度

### 稀疏性

由于少数类样本的稀疏性, 尝试在少数类样本上使用**聚类算法**如k-means, 来得到少数类样本的sub-concepts, 得到少数类样本的所有簇. 但并没有明确的理论指导我们应当如何设置初始cluster的个数.

---

**对少数类样本点使用KNN划分**

在[Types of minority class examples and their influence on learning classifiers from imbalanced data](https://link.springer.com/article/10.1007%2Fs10844-015-0368-1)这篇论文中, 基于少数类样本的K-NN(论文中设定k=5, 使用kernel-based距离度量), 将少数类样本根据**少数类:多数类**的比例分为4个独立的类型.

- **safe**: (5:0, 4:1)
  - safe意味着该少数类样本处于一个少数类cluster的内部, 是可以安全学习的
- **borderline**: (3:2, 2:3)
  - borderline意味着该少数类样本可能处于分类边界附近
- **rare**: (1:4)
  - rare意味着该少数类样本可能属于一个远离主要分布的迷你簇中, 该簇可能只包含数个点
- **outlier**: (0:5)
  - outlier意味着该数据点被多数类样本包围, 很可能是少数类中的离群点

在得到所有少数类样本点的定位之后, 可以做其他的事情. 一个简单的应用是我们可以直接**为少数类的每个类型加权**.

---

### overlapping

如何估计不平衡数据集中overlapping的程度, 但相关研究数量相当稀少.

### noise/outlier

尝试分离出数据集中的noise/outlier(例如通过训练一组基学习器并进行投票, 最常被误分类的一部份样本被视为噪声从训练集中除去), 但直接丢弃样本带来的影响可能需要讨论.

## Extremely Imbalance

对于极度不平衡的数据集, 往往overlapping与sparsity特别严重, 使用上述的对少数类样本分析的方法往往不可行. 例如使用K-NN得到的少数类样本类型几乎全是outlier/rare.

在Extremely Imbalance的实际问题中, 由于严重的sparsity, 想要从训练集中完全准确预测测试集几乎是不可能的任务.
