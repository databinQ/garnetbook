**[RNN梯度消失和爆炸的原因](https://zhuanlan.zhihu.com/p/28687529)**

这篇文章从公式角度出发, 解释了传统RNN极易出现**梯度消失**和**梯度爆炸**的问题.

这里的梯度消失与爆炸指的是按时间维度展开, 即远距离影响(特征)容易消失和特别强(以至于完全覆盖了近距离上下文的影响).

**解决梯度爆炸问题**:

- 常用的方法是对更新梯度进行**截断**(clip), 每步更新的梯度不能大于某个阈值.

**解决梯度消失问题**:

- 最直接的方法是将**激活函数**改为**ReLU函数**. 因为消失的一个很重要的原因是**sigmoid函数**和**tanh函数**的导数基本都是小于1的, 随着时间步的增加梯度自然就消失了. ReLU函数的梯度为0或1, 就不存在这个问题
- 最有效的方法是使用**LSTM**, **GRU**等改进的结构代替RNN. 实际上, 原始的RNN基本也不会使用, 提到RNN基本默认为LSTM结构

---

**[为什么相比于RNN, LSTM在梯度消失上表现更好](https://www.zhihu.com/question/44895610)**

**[Why LSTMs Stop Your Gradients From Vanishing: A View from the Backwards Pass](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html)**

从数学原理上解释了LSTM为什么能解决梯度消失问题(不能解决梯度爆炸问题). 问题的关键就在于后向传播时, **细胞状态**(cell state)之间偏导连乘在原始RNN中随着时间步的增加趋近于0, 但LSTM通过**门结构**(gate)解决里这个问题. 通过训练过程中学习到的参数, 解决了远距离梯度消失的问题, 也即长短距离记忆问题.