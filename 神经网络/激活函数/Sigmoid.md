**Sigmoid**是非常熟悉的激活函数了, 得到的输出值域在$$[0, 1]$$之间, 函数如下:

$$\operatorname{sigmoid}(x)=\sigma=\frac{1}{1+e^{-x}}$$

对应的导数为:

$$\operatorname{sigmoid}^{'}(x) = \sigma (1 - \sigma)$$

对应的函数图和导数图为:

![Sigmoid](img/sigma-sigma-prime.jpg)

## Sigmoid与梯度消失

使用Sigmoid函数往往会带来严重的梯度消失问题, 原因主要有两点:

1. 饱和区域. 从图中可以看到, sigmoid函数大致由一段线性段和两个常数段组成, 大段定义域都落在饱和区域, 对应的导数为零, 参数也就不再更新了

2. sigmoid梯度消失问题严重还有一个原因, 梯度太小. 从导数图中可以看到, 导数的最大值只有0.25, 因此根据链式求导法则, 更新网络中靠前layer的参数时, 由于导数的连乘作用, 得到的梯度会非常的小, 因此, sigomid激活函数**不能使用在较深的网络中**, 一般在输出层或接近输出层中使用.

## Sigmoid函数的优缺点

### 优点

- 梯度平滑
- 输出值在$$[0,1]$$之间

### 缺点

- 计算量大, 正反向传播都包含**幂计算**和**除法**
- 梯度消失, 原因见上
- sigmoid函数不是**zero-centered**, 相关的说明参考[零中心问题](零中心问题.md)
