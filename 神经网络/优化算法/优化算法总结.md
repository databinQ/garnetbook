深度学习优化算法经历了 **SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam** 发展历程. 所有的这些优化算法, 有着相似的形式. 可以使用同一个框架表示.

首先定义符号. $$w$$为待优化参数, $$f(w)$$为目标函数, $$\alpha$$为初始学习率.

在每步$$t$$中:

1. 计算目标函数关于当前参数$$w_t$$的梯度: $$g_{t}=\nabla f\left(w_{t}\right)$$
2. 根据历史梯度$$g_{1}, g_{2}, \cdots, g_{t}$$计算一阶动量和二阶动量: $$m_{t}=\phi\left(g_{1}, g_{2}, \cdots, g_{t}\right)$$; $$V_{t}=\psi\left(g_{1}, g_{2}, \cdots, g_{t}\right)$$
3. 计算当前时刻的下降梯度: $$\eta_{t}=\alpha \cdot m_{t} / \sqrt{V_{t}}$$
4. 根据下降梯度更新参数: $$w_{t+1}=w_{t}-\eta_{t}$$

对于各种各样的优化算法, 第三步, 第四步基本是一致的, 主要差别体现在1和2上.

下面是常见的优化算法表现的演示.

![](img/contours_evaluation_optimizers.gif)

上图是各种优化算法在损失函数平面上的表现.

![](img/saddle_point_evaluation_optimizers.gif)

上图是各种优化算法在遭遇鞍点(梯度为0, Hessian矩阵有正的和负的特征值, 不定)时的表现. 鞍点是各种优化算法共同面临的难点.

# 参考资料

- [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)
- [一个框架看懂优化算法之异同 SGD/AdaGrad/Adam](https://zhuanlan.zhihu.com/p/32230623)
- [深度学习优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)
