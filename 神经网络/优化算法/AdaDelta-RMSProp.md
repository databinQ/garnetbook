将优化算法的框架搬来. 首先定义符号. $$w$$为待优化参数, $$f(w)$$为目标函数, $$\alpha$$为初始学习率.

在每步$$t$$中:

1. 计算目标函数关于当前参数$$w_t$$的梯度: $$g_{t}=\nabla f\left(w_{t}\right)$$
2. 根据历史梯度$$g_{1}, g_{2}, \cdots, g_{t}$$计算一阶动量和二阶动量: $$m_{t}=\phi\left(g_{1}, g_{2}, \cdots, g_{t}\right)$$; $$V_{t}=\psi\left(g_{1}, g_{2}, \cdots, g_{t}\right)$$
3. 计算当前时刻的下降梯度: $$\eta_{t}=\alpha \cdot m_{t} / \sqrt{V_{t}}$$
4. 根据下降梯度更新参数: $$w_{t+1}=w_{t}-\eta_{t}$$

# AdaDelta

Adadelta是对Adagrad的扩展. Adagrad计算二阶动量的方法是将至今所有时间的梯度的平方和累加起来:

$$V_{t}=\sum_{\tau=1}^{t} g_{\tau}^{2}=V_{t-1} + g_{t}^{2}$$

造成了学习率单调递减的问题. 在AdaDelta中, 改变了二阶动量的计算方法, 不再累计全部的历史梯度, 而是只关注一个时间窗口的梯度. 类似于动量的累计方式, 使用**指数移动平均值**计算二阶累计动量:

$$V_{t}=\beta_{2} * V_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}$$

这样就避免了二阶动量持续累积, 导致训练过程提前结束的问题. 一般来说在训练的初期, 中期, 训练很快, 但在后期容易出现**反复抖动**的问题.
