# 背景

NLP中有很多常用的, 与单词, 短语, 句子相关的统计量, 在一些任务中, 特别是无监督任务中, 发挥着重要的作用. 此外, 这些统计值也可以作为**统计特征**, 为神经网络模型补充额外的信息, 如使用`Wide & Deep`, `Deep cross network`网络等.

这些统计值常常使用在无监督方法进行**分词**, **新词发现**, **领域词发现**, **Chunking Task**等任务中. 经常使用的统计值可以分为以下几类.

## 流行度

衡量流行度没有什么特别的方法, 使用**频数**即可. 例如使用一个流行度阈值, 过滤掉出现次数达不到阈值的序列, 认为没有分析的意义.

## 聚合度

聚合度是衡量词或短语(应对不同任务)内部元素(字/互斥子序列)一致性的指标, 也可以成为凝固度, 凝聚度. 例如"齐天"和"大圣"这两个字序列通常都会一起出现, 因此"齐天大圣"这个词的凝聚度很高. 再如关于影评的语料中, "的电影"和"电影院"出现的频数都很高, 但我们知道"电影院"才是真正的实体, 对应的聚合度应当很高.

如何评价聚合度? 直观上, 假设所有的字(子序列)在语料中是随机出现的, 如果两个子序列如"电影"和"院"真的组不成一个词, 那么两者应当是**各自独立地**在文本中随机出现, 那么它们连在一起出现的概率就应该约等于两者的概率相乘. 但如果组合在一起出现的概率远大于两者概率相乘, 我们则认为组合在一起出现不是随机的, 而"电影院"应当为一个词.

计算的方法在上面的说明中已经大致说明. 具体来说, 衡量聚合度有以下的统计值.

### 互信息

$$
\begin{aligned}
\text{MI}(a,b) &= \frac{P(a,b)}{P(a)P(b)} \\
&= \frac{N \cdot \#(a,b)}{\#a \cdot \#b}
\end{aligned}
$$

a, b分别表示两个字/子序列, (a, b)为两者连在一起组成的序列. $$\#(\cdot)$$表示序列出现的频数. 在计算概率时, 不管`n-gram`中的`n`为多少, 序列的总数量与`unigram`相差数量为$$n-1$$, 可以忽略不计, 因此在计算时都使用单字的总数量$$N$$即可.

对于多于两个字的序列, 将其切分为两个子序列时就有不同的切法. 因此在计算时, 需要遍历所有切分方法, 对所有的方案计算对应的互信息, 然后取其中的最小值作为这个序列的互信息值.

在实际工程中, 对于计算过程中使用到的概率最好**取对数**之后再存储使用, 主要考虑到:

- 避免概率过低造成下溢出
- 将取值范围映射到更平滑的区间中

参考:

- [互联网时代的社会语言学：基于SNS的文本数据挖掘](http://www.matrix67.com/blog/archives/5044)
- [基于凝聚度和自由度的非监督词库生成](http://zhanghonglun.cn/blog/project/%E5%9F%BA%E4%BA%8E%E5%87%9D%E8%81%9A%E5%BA%A6%E5%92%8C%E8%87%AA%E7%94%B1%E5%BA%A6%E7%9A%84%E9%9D%9E%E7%9B%91%E7%9D%A3%E8%AF%8D%E5%BA%93%E7%94%9F%E6%88%90/)

### KL散度

这里指的是point-wise Kullback-Leibler divergence. KL散度, 即相对熵, 是描述**两个概率分布**之间差异的一种方式, 差异越大, KL散度越大, 是一种广义的距离.

而且, 我们认为如果一个短语的一致性比较高, 这个短语的左右两部分应当是紧密联系的, 共现的概率一定要远大于随机组合而拼在一起的概率. 因此, 我们衡量这两个概率分布之间的差值, 就能评价这个候选短语的一致性.

对应的公式如下, 从公式来看, 相当于互信息又乘了一个候选短语$$v$$出现的概率, 这样减缓了互信息中对低频短语的衡量偏差.

$$P K L\left(v \|\left\langle u_{l}, u_{r}\right\rangle\right)=p(v) \log \frac{p(v)}{p\left(u_{l}\right) p\left(u_{r}\right)}$$
